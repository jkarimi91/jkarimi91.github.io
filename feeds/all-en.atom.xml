<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jay Karimi</title><link href="https://jkarimi91.github.io/" rel="alternate"></link><link href="https://jkarimi91.github.io/feeds/all-en.atom.xml" rel="self"></link><id>https://jkarimi91.github.io/</id><updated>2018-07-04T00:00:00-07:00</updated><entry><title>Shannon Entropy, Cross Entropy and KL-Divergence</title><link href="https://jkarimi91.github.io/blog/shannon-entropy-cross-entropy-and-kl-divergence" rel="alternate"></link><published>2018-07-04T00:00:00-07:00</published><updated>2018-07-04T00:00:00-07:00</updated><author><name>Jay Karimi</name></author><id>tag:jkarimi91.github.io,2018-07-04:/blog/shannon-entropy-cross-entropy-and-kl-divergence</id><summary type="html">&lt;p&gt;Imagine that there are two entities, one that sends and one that receives messages. Furthermore, imagine that the messages sent by the sender informs the receiver about the occurrence of an event.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sender/Receiver image" src="../images/sender_receiver.png"&gt;&lt;/p&gt;
&lt;p&gt;We can calculate how much information is transferred from the sender to the receiver, via a message, by â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Imagine that there are two entities, one that sends and one that receives messages. Furthermore, imagine that the messages sent by the sender informs the receiver about the occurrence of an event.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sender/Receiver image" src="../images/sender_receiver.png"&gt;&lt;/p&gt;
&lt;p&gt;We can calculate how much information is transferred from the sender to the receiver, via a message, by computing how surprising the receiver finds the occurrence of the event described in the message,&lt;/p&gt;
&lt;div class="math"&gt;$$ S(p) = \log_2 \left(\frac{1}{p}\right) $$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(S(p)\)&lt;/span&gt; is known as the &lt;strong&gt;surprisal&lt;/strong&gt; where &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the probability the receiver associates with the event occurring. We can see that for surprising events, &lt;span class="math"&gt;\(p\)&lt;/span&gt; will be small such that &lt;span class="math"&gt;\(S(p)\)&lt;/span&gt; will be large and vice versa. Let's make this a more concrete with an example.&lt;/p&gt;
&lt;p&gt;Imagine that the head chef, of a restaurant, tells a server that they will be serving fish for dinner. In this example, we can view the chef as the sender, the server as the receiver and the message as the fact that the restaurant will be serving fish for dinner.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chef/Waiter image" src="../images/chef_waiter.png"&gt;&lt;/p&gt;
&lt;p&gt;If the server already knew that fish was going to be on the menu, i.e. &lt;span class="math"&gt;\(p=1\)&lt;/span&gt;, then there will be no surprise and, therefore, no information transferred,&lt;/p&gt;
&lt;div class="math"&gt;$$ S(p_{fish}=1) = \log_2 \left(1\right) = 0 \text{ bits}$$&lt;/div&gt;
&lt;p&gt;But what if the server did not know that they will serving fish for dinner? Perhaps, going into the conversation, the server figured that there was a 75% chance of serving fish, 10% chance of serving chicken and a 15% chance of serving tofu. In this case, the information transferred will be,&lt;/p&gt;
&lt;div class="math"&gt;$$ S(p_{fish}=0.75) = \log_2 \left(\frac{1}{0.75}\right) = 0.415 \text{ bits} $$&lt;/div&gt;
&lt;p&gt;Let's go one step further and imagine that the server hasn't spoken to the chef yet. While we can't compute the amount of information transferred in the conversation, since it hasn't happened yet, we can compute the expected amount of information transferred aka the &lt;strong&gt;Shannon entropy&lt;/strong&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$ \mathbb{E}[I(P)] = H(P) = \sum_{p \in P} pS(p) = 1.05 \text{ bits} $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(P=\{p_{fish}, p_{chicken}, p_{tofu}\}\)&lt;/span&gt; i.e. &lt;span class="math"&gt;\(P\)&lt;/span&gt; is the set containing the probabilities associated with each item being served for dinner.&lt;/p&gt;
&lt;p&gt;At this point, we have looked at how much information a message transfers but we have not looked at how much information is used to encode the message i.e. the message size. To illustrate this difference, let's assume the chef decides to tell the server that what will be served for dinner by repeating it 5 times,&lt;/p&gt;
&lt;div class="math"&gt;$$ message = \text{"fishfishfishfishfish"} $$&lt;/div&gt;
&lt;p&gt;With this encoding scheme, the chef is still transferring &lt;span class="math"&gt;\(S(p_{fish})=0.415 \text{ bits}\)&lt;/span&gt; of information but using 5 times more text than a scheme that doesn't have the repetition.&lt;/p&gt;
&lt;p&gt;In general, we can compute the expected message size, known as the &lt;strong&gt;cross entropy&lt;/strong&gt;, as follows,&lt;/p&gt;
&lt;div class="math"&gt;$$ H(P, Q) = \sum_{i}^N p_i S(q_i) $$&lt;/div&gt;
&lt;p&gt;where the probability of the &lt;span class="math"&gt;\(i^{th}\)&lt;/span&gt; event occurring is given by &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; while the probability of its corresponding encoded sequence occurring is given by &lt;span class="math"&gt;\(q_i\)&lt;/span&gt;.
For example, if the chef decides to use a 2 bit encoding scheme then the messages could be encoded as follows,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two bit encoding image" src="../images/two_bit.png"&gt;&lt;/p&gt;
&lt;p&gt;This encoding scheme implicitly implies that each sequence has an equal chance of occurring. Since there are &lt;span class="math"&gt;\(2^2\)&lt;/span&gt; possible sequences, there is a &lt;span class="math"&gt;\(\frac{1}{2^2} = 25\%\)&lt;/span&gt; chance of any single sequence occurring i.e. &lt;span class="math"&gt;\(q_i = 25\%\)&lt;/span&gt;. As a result,&lt;/p&gt;
&lt;div class="math"&gt;$$ H(P, Q) = 0.75S(0.25) + 0.15S(0.25) + 0.1S(0.25) $$&lt;/div&gt;
&lt;div class="math"&gt;$$ = S(0.25) = 2 \text{ bits} $$&lt;/div&gt;
&lt;p&gt;Notice that the message used more information than it transferred. The difference between how much information is encoded and how much is transferred is known as the &lt;strong&gt;KL-Divergence&lt;/strong&gt;,&lt;/p&gt;
&lt;div class="math"&gt;$$ D_{KL}(P || Q) = H(P, Q) - H(P) = 2 \text{ bits} - 1.05 \text{ bits} = 0.95 \text{ bits} $$&lt;/div&gt;
&lt;p&gt;An ideal encoding scheme will have a &lt;span class="math"&gt;\(D_{KL}(P || Q) = 0 \text{ bits}\)&lt;/span&gt; i.e. it will encode as much information as it transfers. To create an ideal encoding scheme, we can see that we would need &lt;span class="math"&gt;\(q_i = p_i\)&lt;/span&gt; for every &lt;span class="math"&gt;\(i\)&lt;/span&gt;. So in some sense, &lt;span class="math"&gt;\(D_{KL}(P || Q)\)&lt;/span&gt; actually measures how similar two probability distributions are; the more similar &lt;span class="math"&gt;\(P\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt; are, the smaller &lt;span class="math"&gt;\(D_{KL}(P || Q)\)&lt;/span&gt; is.&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=ErfnhcEV1O8"&gt;A Short Introduction to Entropy, Cross-Entropy and KL-Divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;Entropy(Information Theory)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Self-information"&gt;Self-information&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>